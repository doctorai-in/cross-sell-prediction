{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install xgboost imblearn sklearn pickle pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from dateutil import parser\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from data_preprocessing_v2 import Data_Preprocessing\n",
    "from model_v2 import model_fit, model_blending, feature_importance\n",
    "from model_v2 import model_inference\n",
    "import datetime\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dirs(dir):\n",
    "    try: \n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)  \n",
    "    except OSError:\n",
    "        print('Error: Creating directory to store person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dirs(\"models\")\n",
    "validate_dirs(\"result\")\n",
    "validate_dirs(\"cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train/train.csv')\n",
    "test_df= pd.read_csv('test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataframe\n",
    "train_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the null values\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any duplicate rows \n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the unique values of the dataframe features for identifying possible categorical\n",
    "# and numerical feautures \n",
    "\n",
    "unique_values = []\n",
    "for i in range(len(train_df.columns)):\n",
    "    unique_values.append([train_df.columns[i],train_df.iloc[:,i].nunique(),train_df.iloc[:,i].dtypes])\n",
    "\n",
    "\n",
    "unique_values = pd.DataFrame(unique_values)\n",
    "unique_values.columns = ['Column name', 'Unique_values','Dtypes']\n",
    "#unique_values['datatype'] =unique_values[i].dtypes\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the statistics \n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor  = Data_Preprocessing()\n",
    "train_drop_colums = ['Response','id']\n",
    "test_drop_colums = ['id']\n",
    "label_column = 'Response'\n",
    "key = 'id'\n",
    "custom_encode_col = \"Vehicle_Age\"\n",
    "X, Y = data_processor.data_processing_pipeline(train_df, train_drop_colums , label_column, key, data_type = 'Train', custom_encode_col=custom_encode_col) \n",
    "\n",
    "\n",
    "X_test = data_processor.data_processing_pipeline(test_df, test_drop_colums , label_column, key, data_type = 'Test', custom_encode_col=custom_encode_col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy()\n",
    "df[\"Target\"] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=(15,8)\n",
    "f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5)\n",
    "\n",
    "\n",
    "sns.boxplot('Target',y='Age',data=df, ax=ax1)\n",
    "sns.boxplot('Target',y='Annual_Premium',data=df, ax=ax2)\n",
    "sns.boxplot('Target',y='Region_Code',data=df, ax=ax3)\n",
    "sns.boxplot('Target',y='Policy_Sales_Channel',data=df, ax=ax4)\n",
    "sns.boxplot('Target',y='Vintage',data=df, ax=ax5)\n",
    "\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Target\"\n",
    "hue = \"Target\"\n",
    "plt.rcParams['figure.figsize']=(15,8)\n",
    "g = sns.FacetGrid(df, col= col, hue=hue)\n",
    "g.map(sns.distplot, df.columns[0], hist=True, rug=True)\n",
    "\n",
    "g = sns.FacetGrid(df, col= col, hue=hue)\n",
    "g.map(sns.distplot, df.columns[4], hist=True, rug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Matrix\n",
    "sns.set(style=\"dark\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(120, 17, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost, CatBoost and LGB\n",
    "* `KFold`\n",
    "* `StratifiedShuffleSplit`\n",
    "* `stratifiedKFold`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(result, file_name):\n",
    "    df_sub = pd.read_csv('sample_submission.csv')\n",
    "    df_sub.head()\n",
    "    df_xgb = df_sub.copy()\n",
    "    df_xgb['Response'] = result\n",
    "    df_xgb.head()\n",
    "    df_xgb.to_csv('result/'+ filename +'.csv', index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# model---> xgboost \n",
    "split_type = \"stratifiedsuffleSplit\" # stratifiedsuffleSplit, stratifiedKFold, kfold\n",
    "model_name_xgb = \"xgboost\"\n",
    "probs_xgb, probs_xgb_train, model = model_fit(X, Y, X_test, 5, split_type, model_name_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance \n",
    "plt.style.use('ggplot')\n",
    "plt.subplots(figsize=(15, 4))\n",
    "feature_importance(model, X, model_name_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# model---> catboost \n",
    "split_type = \"stratifiedsuffleSplit\" # stratifiedsuffleSplit, stratifiedKFold, kfold\n",
    "model_name_cb = \"catboost\"\n",
    "probs_cb, probs_cb_train, model = model_fit(X, Y, X_test, 5, split_type, model_name_cb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "feature_importance(model, X, model_name_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# model---> lgb \n",
    "split_type = \"stratifiedsuffleSplit\" # stratifiedsuffleSplit, stratifiedKFold, kfold\n",
    "model_name_lgb = \"lgb\"\n",
    "probs_lgb, probs_lgb_train, model = model_fit(X, Y, X_test, 5, split_type, model_name_lgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "feature_importance(model, X, model_name_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set predict_probs\n",
    "p_cb = probs_cb/5\n",
    "p_xgb = probs_xgb/5 \n",
    "# Train Set Predict_probs \n",
    "p_cb_train = probs_cb_train/5\n",
    "p_xgb_train = probs_xgb_train/5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_v2 import model_blending as blend\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix\n",
    "plt.subplots(figsize=(10, 4))\n",
    "best_w, best_roc = blend(p_xgb_train, p_cb_train, Y)\n",
    "display(best_w)\n",
    "display(best_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = best_w\n",
    "result = w * p_xgb + (1-w) * p_cb\n",
    "filename = \"model_blending_cb_xgb_\"+split_type+\"_best_w-\" + str(w) + \"-\"+ datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "save_result(result, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.5\n",
    "result = w * p_xgb + (1-w) * p_cb\n",
    "filename = \"model_blending_cb_xgb_\"+split_type+\"_best_w-\" + str(w) + \"-\"+ datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "save_result(result, filename)"
   ]
  },
  {
   "source": [
    "# Model Load and Predict, predict_proba"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X,model_prefix, prediction=None, predict_probs=None):  \n",
    "    import pickle, joblib\n",
    "    import glob\n",
    "    pridict = np.zeros(shape=(len(X),))\n",
    "    models_path = glob.glob(model_prefix)\n",
    "    for i, v in enumerate(models_path):\n",
    "        model = joblib.load(v)\n",
    "        if prediction:\n",
    "            result = model.predict(X)\n",
    "        if predict_probs:\n",
    "            result = model.predict_proba(X)    \n",
    "        pridict += result\n",
    "    return pridict    \n"
   ]
  },
  {
   "source": [
    "# Confusion Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost\n",
    "model_path_prefix_xgb = \"models/\" + model_name_xgb + \"/\" + split_type + \"/\" + model_name_xgb +\"_\" + split_type + \"*\"\n",
    "y_pred_xgb = (model_predict(X, model_path_prefix_xgb, prediction=True, predict_probs=False)/5).astype(int) \n",
    "cm_xgb = confusion_matrix(Y, y_pred_xgb)\n",
    "cm_df_xgb = pd.DataFrame(data=cm_xgb, columns=[\"Response_0\", \"Response_1\"], index=[\"Response_0\", \"Response_1\"])\n",
    "cm_df_xgb.to_csv(\"cm/xgb_cm_\" + split_type + \".csv\")\n",
    "print(\"::SAVED XGB CONFUSION MATRIX::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catboost\n",
    "model_path_prefix_cb = \"models/\" + model_name_cb + \"/\" + split_type + \"/\" + model_name_cb +\"_\" + split_type + \"*\"\n",
    "y_pred_cb = (model_predict(X, model_path_prefix_cb, prediction=True, predict_probs=False)/5).astype(int)\n",
    "cm_cb = confusion_matrix(Y, y_pred_cb)\n",
    "cm_df_cb = pd.DataFrame(data=cm_cb, columns=[\"Response_0\", \"Response_1\"], index=[\"Response_0\", \"Response_1\"])\n",
    "cm_df_cb.to_csv(\"cm/catboost_cm_\" + split_type + \".csv\")\n",
    "print(\"::SAVED CATBOOST CONFUSION MATRIX::\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGB\n",
    "model_path_prefix_lgb = \"models/\" + model_name_lgb + \"/\" + split_type + \"/\" + model_name_lgb +\"_\" + split_type + \"*\"\n",
    "y_pred_cb = (model_predict(X, model_path_prefix_lgb, prediction=True, predict_probs=False)/5).astype(int)\n",
    "cm_cb = confusion_matrix(Y, y_pred_lgb)\n",
    "cm_df_cb = pd.DataFrame(data=cm_cb, columns=[\"Response_0\", \"Response_1\"], index=[\"Response_0\", \"Response_1\"])\n",
    "cm_df_cb.to_csv(\"cm/\" + model_name_lgb + \"_cm_\" + split_type + \".csv\")\n",
    "print(\"::SAVED LGB CONFUSION MATRIX::\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}